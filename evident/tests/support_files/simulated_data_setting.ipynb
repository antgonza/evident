{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is to generate the simulated data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simulated mapping files for alpha diversities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general mapping file: 120 subjects with 20 missing values \n",
    "# age -- continous variable; \n",
    "# gender -- categorical variable with 2 levels; \n",
    "# country -- categorical variable with 4 levels\n",
    "\n",
    "np.random.seed(31)\n",
    "mapping = pd.DataFrame(\n",
    "    {'#SampleID': ['10000.000001010' +\n",
    "                   x for x in np.char.mod(\n",
    "                       '%d', np.arange(start=1, stop=121, step=1))],\n",
    "     'Age': np.append(np.sort(np.round(np.random.normal(loc=40, scale=5,\n",
    "                      size=100), decimals=0)), np.repeat('nan', 20)),\n",
    "     'Gender': np.append(np.repeat(np.array(['Male', 'Female']), 50),\n",
    "                         np.repeat('Not applicable', 20)),\n",
    "     'Country': np.append(np.repeat(np.array(['US', 'France',\n",
    "                                              'Germany', 'Mexico']), 25),\n",
    "                          np.repeat('Missing: Not provided', 20))})\n",
    "mapping.set_index('#SampleID', inplace=True)\n",
    "\n",
    "# separate mapping files\n",
    "# age\n",
    "mapping_age = pd.DataFrame({'#SampleID': ['10000.000001010' +\n",
    "                            x for x in np.char.mod(\n",
    "                              '%d', np.arange(start=1, stop=121, step=1))],\n",
    "                            'Age': mapping.Age})\n",
    "mapping_age.set_index('#SampleID', inplace=True)\n",
    "\n",
    "# gender\n",
    "mapping_gender = pd.DataFrame({'#SampleID': ['10000.000001010' +\n",
    "                              x for x in np.char.mod(\n",
    "                                '%d', np.arange(start=1, stop=121, step=1))],\n",
    "                              'Gender': mapping.Gender})\n",
    "mapping_gender.set_index('#SampleID', inplace=True)\n",
    "\n",
    "# country\n",
    "mapping_country = pd.DataFrame({'#SampleID': ['10000.000001010' +\n",
    "                               x for x in np.char.mod(\n",
    "                                 '%d', np.arange(start=1, stop=121, step=1))],\n",
    "                               'Country': mapping.Country})\n",
    "mapping_country.set_index('#SampleID', inplace=True)\n",
    "\n",
    "mapping.to_csv('mappings.txt', sep='\\t')\n",
    "mapping_age.to_csv('mapping_age.txt', sep='\\t')\n",
    "mapping_gender.to_csv('mapping_gender.txt', sep='\\t')\n",
    "mapping_country.to_csv('mapping_country.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simulated alpha diversities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multipe alphas:\n",
    "# Faith_PD ~ Uniform (0, 40): negligible effect size\n",
    "# Shannon: a mixture of N(100, 1) and N(3, 1): largest effect size\n",
    "# Observed_OTUs: a mixture of N(5, 2), N(15, 10), N(20, 8) and N(30, 2): medium effect size\n",
    "# Expected effect size (from smallest to biggest): Faith_PD < Observed_OTUs < Shannon\n",
    "# Expected p-value (from smallest to biggest): Shannon < Observed_OTUs < Faith_PD\n",
    "\n",
    "alpha_div = pd.DataFrame({'#SampleID': ['10000.000001010' +\n",
    "                         x for x in np.char.mod(\n",
    "                           '%d', np.arange(start=1, stop=121, step=1))],\n",
    "                          'Faith_PD': np.append(np.random.uniform(\n",
    "                            low=0, high=40, size=100),\n",
    "                            np.repeat('None', 20)),\n",
    "                          'Shannon': np.concatenate(\n",
    "                            (abs(np.random.normal(loc=100, scale=1, size=60)),\n",
    "                             abs(np.random.normal(loc=3, scale=1, size=60)))),\n",
    "                          'Observed_OTUs': np.concatenate(\n",
    "                            (abs(np.random.normal(loc=5, scale=2, size=30)),\n",
    "                             abs(np.random.normal(loc=15, scale=10, size=30)),\n",
    "                             abs(np.random.normal(loc=20, scale=8, size=30)),\n",
    "                             abs(np.random.normal(\n",
    "                                loc=30, scale=2, size=30))))})\n",
    "alpha_div.set_index('#SampleID', inplace=True)\n",
    "\n",
    "# separate alpha diversity file\n",
    "# Faith_PD\n",
    "alpha_pd = pd.DataFrame({'#SampleID': ['10000.000001010' +\n",
    "                        x for x in np.char.mod(\n",
    "                          '%d', np.arange(start=1, stop=121, step=1))],\n",
    "                         'Faith_PD': alpha_div.Faith_PD})\n",
    "alpha_pd.set_index('#SampleID', inplace=True)\n",
    "\n",
    "# Shannon\n",
    "alpha_sn = pd.DataFrame({'#SampleID': ['10000.000001010' +\n",
    "                        x for x in np.char.mod(\n",
    "                          '%d', np.arange(start=1, stop=121, step=1))],\n",
    "                         'Shannon': alpha_div.Shannon})\n",
    "alpha_sn.set_index('#SampleID', inplace=True)\n",
    "\n",
    "# Observed_OTUs\n",
    "alpha_otu = pd.DataFrame({'#SampleID': ['10000.000001010' +\n",
    "                         x for x in np.char.mod(\n",
    "                           '%d', np.arange(start=1, stop=121, step=1))],\n",
    "                         'Observed_OTUs': alpha_div.Observed_OTUs})\n",
    "alpha_otu.set_index('#SampleID', inplace=True)\n",
    "\n",
    "# output alpha diversity files\n",
    "alpha_div.to_csv('alphas.txt', sep='\\t')\n",
    "alpha_pd.to_csv('alpha_pd.txt', sep='\\t')\n",
    "alpha_sn.to_csv('alpha_sn.txt', sep='\\t')\n",
    "alpha_otu.to_csv('alpha_otu.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simulated mapping files for beta diversities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general mapping file: 120 subjects with 20 missing values \n",
    "# BMI -- continous variable; \n",
    "# Site -- categorical variable with 2 levels; \n",
    "# Race -- categorical variable with 4 levels\n",
    "\n",
    "np.random.seed(31)\n",
    "mapping = pd.DataFrame(\n",
    "    {'#SampleID': np.arange(start=1, stop=121, step=1),\n",
    "     'BMI': np.append(np.sort(np.round(np.random.normal(loc=20, scale=5,\n",
    "                      size=100), decimals=0)), np.repeat('nan', 20)),\n",
    "     'Site': np.append(np.repeat(np.array(['US', 'UK']), 50),\n",
    "                       np.repeat('Not applicable', 20)),\n",
    "     'Race': np.append(np.repeat(np.array(['White', 'Hispanics',\n",
    "                                           'Asian', 'Others']), 25),\n",
    "                       np.repeat('Missing: Not provided', 20))})\n",
    "mapping.set_index('#SampleID', inplace=True)\n",
    "\n",
    "# separate mapping files\n",
    "# bmi\n",
    "mapping_bmi = pd.DataFrame({'#SampleID': np.arange(start=1, stop=121, step=1),\n",
    "                            'BMI': mapping.BMI})\n",
    "mapping_bmi.set_index('#SampleID', inplace=True)\n",
    "\n",
    "# site\n",
    "mapping_site = pd.DataFrame({'#SampleID': np.arange(start=1, stop=121, step=1),\n",
    "                             'Site': mapping.Site})\n",
    "mapping_site.set_index('#SampleID', inplace=True)\n",
    "\n",
    "# race\n",
    "mapping_race = pd.DataFrame({'#SampleID': np.arange(start=1, stop=121, step=1),\n",
    "                             'Race': mapping.Race})\n",
    "mapping_race.set_index('#SampleID', inplace=True)\n",
    "\n",
    "mapping.to_csv('mappings.txt', sep='\\t')\n",
    "mapping_bmi.to_csv('mapping_bmi.txt', sep='\\t')\n",
    "mapping_site.to_csv('mapping_site.txt', sep='\\t')\n",
    "mapping_race.to_csv('mapping_race.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simulated beta diversities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multipe betas (based on 2 simulated taxa):\n",
    "# dist_site: two sites with completely different taxa, resulting in large distance\n",
    "# dist_race: two races have completely different taxa, the other two the same\n",
    "# dist_bmi: no difference among different bmis\n",
    "# Expected effect size (from smallest to biggest): dist_bmi < dist_race < dist_site\n",
    "# Expected p-value (from smallest to biggest): dist_site < dist_race < dist_bmi\n",
    "\n",
    "taxa_site = pd.DataFrame({'#SampleID': np.arange(start=1, stop=121, step=1),\n",
    "                          'Taxa1': np.repeat(np.array([1000, 10]), 60),\n",
    "                          'Taxa2': np.repeat(np.array([10, 1000]), 60)})\n",
    "taxa_site.set_index('#SampleID', inplace=True)\n",
    "\n",
    "dist_site = pd.DataFrame(distance_matrix(taxa_site.values,\n",
    "                                         taxa_site.values),\n",
    "                         index=taxa_site.index, columns=taxa_site.index)\n",
    "dist_site.to_csv('dist_site.txt', sep='\\t')\n",
    "\n",
    "taxa_race = pd.DataFrame({'#SampleID': np.arange(start=1, stop=121, step=1),\n",
    "                          'Taxa1': np.repeat(np.array([1000, 100,\n",
    "                                                       500, 0]), 30),\n",
    "                          'Taxa2': np.repeat(np.array([0, 100, 500,\n",
    "                                                       1000]), 30)})\n",
    "taxa_race.set_index('#SampleID', inplace=True)\n",
    "\n",
    "dist_race = pd.DataFrame(distance_matrix(taxa_race.values,\n",
    "                                         taxa_race.values),\n",
    "                         index=taxa_race.index, columns=taxa_race.index)\n",
    "dist_race.to_csv('dist_race.txt', sep='\\t')\n",
    "\n",
    "taxa_bmi = pd.DataFrame({'#SampleID': np.arange(start=1, stop=121, step=1),\n",
    "                         'Taxa1': np.round(np.random.uniform(low=0,\n",
    "                                           high=100, size=120), decimals=0),\n",
    "                         'Taxa2': np.round(np.random.uniform(low=0,\n",
    "                                           high=100, size=120), decimals=0)})\n",
    "taxa_bmi.set_index('#SampleID', inplace=True)\n",
    "\n",
    "dist_bmi = pd.DataFrame(distance_matrix(taxa_bmi.values,\n",
    "                                        taxa_bmi.values),\n",
    "                        index=taxa_bmi.index, columns=taxa_bmi.index)\n",
    "dist_bmi.to_csv('dist_bmi.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
